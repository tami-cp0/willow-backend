import { Product } from "@prisma/client";
import { getVisionResponse, instantiateModel } from "../config/aiConfig";

const sustainabilityPrompt = (d: {
    cloudVisionRes: string;
    sourcing: string;
    name: string;
    description: string;
    category: string;
    sf: string[];
    packaging: string;
    price: number;
    onDemand: boolean;
    eol: string | null;
    options: any;
    inStock: number | null;
    certification?: any;
  }) => `
**You are a sustainability vetting agent for Willow, an eco-conscious marketplace.**  
Your task is to evaluate product listings using both seller-provided data (the only information available from the seller) and Google Cloud Vision analysis. Remember: **no product is perfectly sustainable**, and trade-offs always exist.

### Core Instructions
- **Data Sources:**  
  - Evaluate seller-provided product data and Cloud Vision results equally.  
  - Validate or challenge sustainability claims based solely on the provided information.
  
- **Seller Bias:**  
  - Be OBJECTIVE and do not be swayed by overly positive or "sweet" language in the seller data. Use the data and Cloud Vision analysis to confirm or refute the claims.
  
- **Known Brands:**  
  - Incorporate publicly available brand sustainability data when evaluating recognized brands.
  
- **Handling Data Limitations:**  
  - Do not penalize for missing details unless the gap is critical.
  - If the product data does not offer enough reliable evidence to verify a claim (for instance, when a claim about material authenticity or composition cannot be confirmed through text or images alone), mark the listing as **Inconclusive** (Score: 0) with an explanation.

### Evaluation Protocol
1. **Mismatch Check:**  
   - Compare the seller’s product description with Cloud Vision results.  
   - **If a contradiction exists (e.g., the description and images do not align):**  
     - **Score:** 0.5  
     - **Tag:** DETAILS_MISMATCH  
     - **Explanation:** Describe the mismatch and cease further evaluation.

2. **Positive and Negative Factors:**  
   - Identify and verify seller-claimed sustainability features (e.g., recyclable packaging, local sourcing) using all available data.
  
3. **Data Limitations & Image Quality:**  
   - Mark evaluations as **Inconclusive** (Score: 0) if the provided data does not offer sufficient reliable evidence to verify a claim (for example, if material or composition claims cannot be confirmed).

### Score Ranges & Output Format
- **Score 90–100 (Great):** Exceptional sustainability, fully verifiable.  
- **Score 70–89 (Good):** Strong efforts with minor limitations.  
- **Score 50–69 (It’s a Start):** Moderate features with notable trade-offs.  
- **Score 30–49 (We Avoid):** Minimal sustainability benefits.  
- **Score 1–29 (Not Good Enough):** Negligible or harmful sustainability efforts.  
- **Score 0.5 (Mismatch):** For contradictory information.  
- **Score 0 (Inconclusive):** When the product data does not provide enough reliable evidence to verify a claim.

**Output (no formatting):**  
- **Sustainability Score:** [0–100, 0, or 0.5]  
- **Sustainability Tag:** [One key tag from the seller-selected features]  
- **Explanation:** A concise 2–4 sentence summary highlighting positives, trade-offs, and any limitations in the data.

### Product Data Input Variables
*(This is the only information a seller can provide)*  
- **Product Name:** ${d.name}  
- **Description:** ${d.description}  
- **Category:** ${d.category}  
- **Price (NGN):** ${d.price}  
- **In-Stock:** ${d?.inStock}  
- **On-Demand:** ${d.onDemand}  
- **Options:** ${d.options}  
- **Production Location:** ${d.sourcing}  
- **Packaging:** ${d.packaging}  
- **Seller-Selected Sustainability Features:** ${d.sf}  
- **End-of-Life Considerations:** ${d?.eol}  
- **Google Cloud Vision Results:** ${d.cloudVisionRes}  
`;

// ALL INTERFACES WERE GENERATED BY AI, no time fr.
interface GoogleVisionColor {
   color?: {
     red?: number;
     green?: number;
     blue?: number;
     alpha?: number;
   };
   score?: number;
   pixelFraction?: number;
 }
 
 interface GoogleVisionDominantColors {
   colors?: GoogleVisionColor[];
 }
 
 interface GoogleVisionImagePropertiesAnnotation {
   dominantColors?: GoogleVisionDominantColors;
 }
 
 interface GoogleVisionTextAnnotation {
   locale?: string;
   description?: string;
   boundingPoly?: any; // Define a more specific type if needed
 }
 
 interface GoogleVisionLabelAnnotation {
   description?: string;
   score?: number;
   confidence?: number;
   topicality?: number;
   boundingPoly?: any; // Define a more specific type if needed
 }
 
 interface GoogleVisionResponse {
   textAnnotations?: GoogleVisionTextAnnotation[];
   imagePropertiesAnnotation?: GoogleVisionImagePropertiesAnnotation;
   labelAnnotations?: GoogleVisionLabelAnnotation[];
   // Add other fields from the Google Cloud Vision API response if you need them
 }
 
 interface RawGoogleVisionJson {
   responses?: GoogleVisionResponse[];
   // Add other fields from the top-level JSON response if needed
 }
 
 interface ImageSummary {
   text: string | null;
   dominantColors: GoogleVisionColor[];
   objectLabels: string[];
 }
 
 function summarizeCloudVisionOutput(cloudVisionJson: RawGoogleVisionJson | null | undefined): ImageSummary[] {
   if (!cloudVisionJson || !cloudVisionJson.responses) {
     return []; // Return empty array if no valid JSON or responses
   }
 
   return cloudVisionJson.responses.map((response: GoogleVisionResponse) => {
     const imageSummary: ImageSummary = {
       text: null,
       dominantColors: [],
       objectLabels: [],
     };
 
     // 1. Extract Text Annotations
     if (response.textAnnotations && response.textAnnotations.length > 0) {
       imageSummary.text = response.textAnnotations[0].description || null; // Use first annotation's description, handle undefined
     }
 
     // 2. Extract Dominant Colors
     if (response.imagePropertiesAnnotation && response.imagePropertiesAnnotation.dominantColors && response.imagePropertiesAnnotation.dominantColors.colors) {
       imageSummary.dominantColors = response.imagePropertiesAnnotation.dominantColors.colors;
     }
 
     // 3. Extract Object Labels (using labelAnnotations)
     if (response.labelAnnotations) {
       imageSummary.objectLabels = response.labelAnnotations.map(label => label.description || ''); // Map descriptions, handle undefined
     }
 
     return imageSummary;
   });
 }

type ProductImage = {
    url: string;
    key: string;
    size: number;
    mimetype: string;
    originalname: string;
};

const model = instantiateModel("gemini-1.5-flash");

async function vetProduct(product: Product) {
    try {
        const images = product.images as ProductImage[];
        
        const imageUrls = images.map(image => image.url);

        const formatted: ImageSummary[] = summarizeCloudVisionOutput(await getVisionResponse(imageUrls)); 
    
        const result = (
          await model.generateContent([
            sustainabilityPrompt({
              category: product.category,
              cloudVisionRes: JSON.stringify(formatted),
              description: product.description,
              sourcing: product.sourcing,
              name: product.name,
              onDemand: product.onDemand,
              packaging: product.packaging,
              price: product.price,
              sf: product.sustainabilityFeatures,
              eol: product.endOfLifeInfo,
              inStock: product.inStock,
              options: product.options,
              certification: product.certification
            }),
          ])
        ).response.text();
    
        return result;
      } catch (error) {
        console.log(`vet product Error (id: ${product.id}): `, error);
        throw error;
      }
}

export default vetProduct;
